{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym_chess\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import cast, List, Tuple, Deque, Optional, Callable\n",
    "\n",
    "env = gym.make(\"ChessVsSelf-v1\", log=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input array.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : np.ndarray\n",
    "        The input array for which to compute the sigmoid function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The output array with the sigmoid function applied element-wise.\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Logistic Regression ############################################\n",
    "\n",
    "\n",
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic Regression model for binary classification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    observations_size : int\n",
    "        The size of the observation space.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    observation_size : int\n",
    "        The size of the observation space.\n",
    "    params : np.ndarray\n",
    "        The parameters of the logistic regression model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observations_size: int):\n",
    "        \"\"\"\n",
    "        Initialize the LogisticRegression model with random parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observations_size : int\n",
    "            The size of the observation space.\n",
    "        \"\"\"\n",
    "        self.observation_size = observations_size\n",
    "        self.params = np.random.rand(observations_size)\n",
    "\n",
    "    def __call__(self, observation: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Predict the class label for a given observation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : np.ndarray\n",
    "            The input observation.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The predicted class label (0 or 1).\n",
    "        \"\"\"\n",
    "        prob_push_right = sigmoid(np.dot(observation, np.transpose(self.params)))\n",
    "        \n",
    "        return 1 if np.random.rand() < prob_push_right else 0\n",
    "\n",
    "    def get_params(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the parameters of the logistic regression model.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            The parameters of the logistic regression model.\n",
    "        \"\"\"\n",
    "        return self.params.copy()\n",
    "\n",
    "    def set_params(self, params: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Set the parameters of the logistic regression model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : np.ndarray\n",
    "            The parameters of the logistic regression model.\n",
    "        \"\"\"\n",
    "        self.params = params.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(\n",
    "    env: gym.Env, policy: torch.nn.Module, num_episode: int = 1, num_steps: int = 100\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Test a naive agent in the given environment using the provided Q-network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment in which to test the agent.\n",
    "    policy : torch.nn.Module\n",
    "        The neural network to use for decision making.\n",
    "    num_episode : int, optional\n",
    "        The number of episodes to run, by default 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of rewards per episode.\n",
    "    \"\"\"\n",
    "    collected_rewards = []\n",
    "\n",
    "    for episode_id in range(num_episode):\n",
    "        observation = env.reset()\n",
    "        print(\"\\n\", \"=\" * 10, \"NEW GAME\", \"=\" * 10)\n",
    "        env.render()\n",
    "        total_rewards = {\"WHITE\": 0, \"BLACK\": 0}\n",
    "\n",
    "        \n",
    "        for j in range(num_steps):\n",
    "            \n",
    "            observation = observation.flatten()\n",
    "            \n",
    "            action = policy(observation)\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_rewards[\"WHITE\"] += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # black moves\n",
    "            moves = env.possible_moves\n",
    "            m = random.choice(moves)\n",
    "            a = env.move_to_action(m)\n",
    "            # perform action\n",
    "            state, reward, done, _ = env.step(a)\n",
    "            total_rewards[\"BLACK\"] += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            observation = state\n",
    "\n",
    "        print(\">\" * 5, \"GAME\", i, \"REWARD:\", total_rewards)\n",
    "        collected_rewards.append(total_rewards)\n",
    "\n",
    "\n",
    "    return collected_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ========== NEW GAME ==========\n",
      "    -------------------------\n",
      " 8 |  ♖  ♘  ♗  ♕  ♔  ♗  ♘  ♖ |\n",
      " 7 |  ♙  ♙  ♙  ♙  ♙  ♙  ♙  ♙ |\n",
      " 6 |  .  .  .  .  .  .  .  . |\n",
      " 5 |  .  .  .  .  .  .  .  . |\n",
      " 4 |  .  .  .  .  .  .  .  . |\n",
      " 3 |  .  .  .  .  .  .  .  . |\n",
      " 2 |  ♟  ♟  ♟  ♟  ♟  ♟  ♟  ♟ |\n",
      " 1 |  ♜  ♞  ♝  ♛  ♚  ♝  ♞  ♜ |\n",
      "    -------------------------\n",
      "      a  b  c  d  e  f  g  h \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m nn_policy \u001b[38;5;241m=\u001b[39m LogisticRegression(\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtest_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 53\u001b[0m, in \u001b[0;36mtest_agent\u001b[1;34m(env, policy, num_episode, num_steps)\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     51\u001b[0m         observation \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGAME\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mi\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREWARD:\u001b[39m\u001b[38;5;124m\"\u001b[39m, total_rewards)\n\u001b[0;32m     54\u001b[0m     collected_rewards\u001b[38;5;241m.\u001b[39mappend(total_rewards)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collected_rewards\n",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "nn_policy = LogisticRegression(64)\n",
    "\n",
    "test_agent(env, nn_policy, num_episode=1, num_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectiveFunction:\n",
    "    \"\"\"\n",
    "    Objective function for evaluating a policy in a given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment in which to evaluate the policy.\n",
    "    policy : torch.nn.Module\n",
    "        The policy to evaluate.\n",
    "    num_episodes : int, optional\n",
    "        The number of episodes to run for each evaluation, by default 1.\n",
    "    max_time_steps : float, optional\n",
    "        The maximum number of time steps per episode, by default float(\"inf\").\n",
    "    minimization_solver : bool, optional\n",
    "        Whether the solver is a minimization solver, by default True.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment in which to evaluate the policy.\n",
    "    policy : torch.nn.Module\n",
    "        The policy to evaluate.\n",
    "    num_episodes : int\n",
    "        The number of episodes to run for each evaluation.\n",
    "    max_time_steps : float\n",
    "        The maximum number of time steps per episode.\n",
    "    minimization_solver : bool\n",
    "        Whether the solver is a minimization solver.\n",
    "    num_evals : int\n",
    "        The number of evaluations performed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        policy: torch.nn.Module,\n",
    "        num_episodes: int = 1,\n",
    "        max_time_steps: float = float(\"inf\"),\n",
    "        minimization_solver: bool = True,\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.num_episodes = num_episodes\n",
    "        self.max_time_steps = max_time_steps\n",
    "        self.minimization_solver = minimization_solver\n",
    "\n",
    "        self.num_evals = 0\n",
    "\n",
    "    def eval(self, policy_params: np.ndarray, num_episodes: Optional[int] = None, max_time_steps: Optional[float] = None) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate a policy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        policy_params : np.ndarray\n",
    "            The parameters of the policy to evaluate.\n",
    "        num_episodes : int, optional\n",
    "            The number of episodes to run for each evaluation, by default None.\n",
    "        max_time_steps : float, optional\n",
    "            The maximum number of time steps per episode, by default None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The average total rewards over the evaluation episodes.\n",
    "        \"\"\"\n",
    "        self.policy.set_params(policy_params)\n",
    "\n",
    "        self.num_evals += 1\n",
    "\n",
    "        if num_episodes is None:\n",
    "            num_episodes = self.num_episodes\n",
    "\n",
    "        if max_time_steps is None:\n",
    "            max_time_steps = self.max_time_steps\n",
    "\n",
    "        average_total_rewards = 0\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "            total_rewards = {\"WHITE\": 0, \"BLACK\": 0}\n",
    "            observation = self.env.reset()\n",
    "\n",
    "            for t in range(max_time_steps):\n",
    "                observation = observation.flatten()\n",
    "            \n",
    "                action = self.policy(observation)\n",
    "\n",
    "                state, reward, done, _ = env.step(action)\n",
    "                total_rewards[\"WHITE\"] += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                # black moves\n",
    "                moves = env.possible_moves\n",
    "                m = random.choice(moves)\n",
    "                a = env.move_to_action(m)\n",
    "                # perform action\n",
    "                state, reward, done, _ = env.step(a)\n",
    "                total_rewards[\"BLACK\"] += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                observation = state\n",
    "\n",
    "            average_total_rewards += float(total_rewards[\"WHITE\"]) / num_episodes\n",
    "\n",
    "        if self.minimization_solver:\n",
    "            average_total_rewards *= -1.0\n",
    "\n",
    "        return average_total_rewards  # Optimizers do minimization by default...\n",
    "\n",
    "    def __call__(self, policy_params: np.ndarray, num_episodes: Optional[int] = None, max_time_steps: Optional[float] = None) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate a policy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        policy_params : np.ndarray\n",
    "            The parameters of the policy to evaluate.\n",
    "        num_episodes : int, optional\n",
    "            The number of episodes to run for each evaluation, by default None.\n",
    "        max_time_steps : float, optional\n",
    "            The maximum number of time steps per episode, by default None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The average total rewards over the evaluation episodes.\n",
    "        \"\"\"\n",
    "        return self.eval(policy_params, num_episodes, max_time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cem_uncorrelated(\n",
    "    objective_function: Callable[[np.ndarray], float],\n",
    "    mean_array: np.ndarray,\n",
    "    var_array: np.ndarray,\n",
    "    max_iterations: int = 500,\n",
    "    sample_size: int = 50,\n",
    "    elite_frac: float = 0.2,\n",
    "    print_every: int = 10,\n",
    "    success_score: float = float(\"inf\"),\n",
    "    num_evals_for_stop: Optional[int] = None,\n",
    "    hist_dict: Optional[dict] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cross-entropy method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    objective_function : Callable[[np.ndarray], float]\n",
    "        The function to maximize.\n",
    "    mean_array : np.ndarray\n",
    "        The initial proposal distribution (mean vector).\n",
    "    var_array : np.ndarray\n",
    "        The initial proposal distribution (variance vector).\n",
    "    max_iterations : int, optional\n",
    "        Number of training iterations, by default 500.\n",
    "    sample_size : int, optional\n",
    "        Size of population at each iteration, by default 50.\n",
    "    elite_frac : float, optional\n",
    "        Rate of top performers to use in update with elite_frac ∈ ]0;1], by default 0.2.\n",
    "    print_every : int, optional\n",
    "        How often to print average score, by default 10.\n",
    "    success_score : float, optional\n",
    "        The score at which to stop the optimization, by default float(\"inf\").\n",
    "    num_evals_for_stop : Optional[int], optional\n",
    "        Number of evaluations for stopping criteria, by default None.\n",
    "    hist_dict : Optional[dict], optional\n",
    "        Dictionary to log the history, by default None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The optimized mean vector.\n",
    "    \"\"\"\n",
    "    assert 0.0 < elite_frac <= 1.0\n",
    "\n",
    "    n_elite = math.ceil(sample_size * elite_frac)\n",
    "\n",
    "    for iteration_index in range(0, max_iterations):\n",
    "\n",
    "        # SAMPLE A NEW POPULATION OF SOLUTIONS (X VECTORS) ####################\n",
    "\n",
    "        # SAMPLE A NEW POPULATION OF SOLUTIONS (X VECTORS)\n",
    "        x_array = torch.tensor(mean_array) + torch.sqrt(torch.tensor(var_array)) * torch.randn((sample_size, len(mean_array)))\n",
    "\n",
    "\n",
    "        # EVALUATE SAMPLES AND EXTRACT THE BEST ONES (\"ELITE\") ################\n",
    "\n",
    "        score_array = torch.tensor([objective_function(x.numpy(), 1, 100) for x in tqdm(x_array)])\n",
    "\n",
    "        sorted_indices_array = score_array.argsort()       # Sort from the lower score to the higher one\n",
    "        elite_indices_array = sorted_indices_array[:n_elite]              # Recall: we *minimize* the objective function thus we take the samples that are at the begining of the sorted_indices\n",
    "\n",
    "        elite_x_array = x_array[elite_indices_array]\n",
    "\n",
    "        # FIT THE NORMAL DISTRIBUTION ON THE ELITE POPULATION #################\n",
    "\n",
    "        mean_array = elite_x_array.mean(dim=0)\n",
    "        var_array = elite_x_array.var(dim=0)\n",
    "        score = objective_function(mean_array.numpy(), 1, 100)\n",
    "\n",
    "        # PRINT STATUS ########################################################\n",
    "\n",
    "        if iteration_index % print_every == 0:\n",
    "            print(\"Iteration {}\\tScore {}\".format(iteration_index, score))\n",
    "\n",
    "        if hist_dict is not None:\n",
    "            hist_dict[iteration_index] = score\n",
    "\n",
    "        # STOPPING CRITERIA ####################################################\n",
    "\n",
    "        if num_evals_for_stop is not None:\n",
    "            score = objective_function(mean_array.numpy(), num_evals_for_stop)\n",
    "\n",
    "        # `num_evals_for_stop = None` may be used to fasten computations but it introduces bias...\n",
    "        if score <= success_score:\n",
    "            break\n",
    "\n",
    "    return mean_array.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"ChessVsSelf-v1\", log=False)\n",
    "\n",
    "nn_policy = LogisticRegression(64)\n",
    "\n",
    "objective_function = ObjectiveFunction(\n",
    "    env=env, policy=nn_policy, num_episodes=10, max_time_steps=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:25<00:00,  2.56s/it]\n",
      "C:\\Users\\gentl\\AppData\\Local\\Temp\\ipykernel_18880\\370760933.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_array = torch.tensor(mean_array) + torch.sqrt(torch.tensor(var_array)) * torch.randn((sample_size, len(mean_array)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\tScore 1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:08<00:19,  2.83s/it]C:\\Users\\gentl\\AppData\\Local\\Temp\\ipykernel_18880\\2824705607.py:15: RuntimeWarning: overflow encountered in exp\n",
      "  return 1.0 / (1.0 + np.exp(-x))\n",
      "100%|██████████| 10/10 [00:26<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\tScore 980.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:25<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 2\tScore 980.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:27<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3\tScore 980.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 4\tScore 980.0\n"
     ]
    }
   ],
   "source": [
    "hist_dict = {}\n",
    "\n",
    "num_params = len(nn_policy.get_params())\n",
    "\n",
    "init_mean_array = np.random.random(num_params)\n",
    "init_var_array = np.ones(num_params) * 100.0\n",
    "\n",
    "optimized_policy_params = cem_uncorrelated(\n",
    "    objective_function=objective_function,\n",
    "    mean_array=init_mean_array,\n",
    "    var_array=init_var_array,\n",
    "    max_iterations=5,\n",
    "    sample_size=10,\n",
    "    elite_frac=0.2,\n",
    "    print_every=1,\n",
    "    success_score=-500,\n",
    "    num_evals_for_stop=None,\n",
    "    hist_dict=hist_dict,\n",
    ")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = objective_function(optimized_policy_params, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "960.0\n"
     ]
    }
   ],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(\n",
    "    hist_dict,\n",
    "    orient=\"index\",\n",
    "    columns=[\"score\", \"mu1\", \"mu2\", \"mu3\", \"mu4\", \"var1\", \"var2\", \"var3\", \"var4\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
